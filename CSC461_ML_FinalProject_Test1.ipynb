{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "import pickle\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import drive\n",
        "\n",
        "# Initialize the path to the shared folder globally\n",
        "data_path = '/content/drive/MyDrive/ML final project'\n",
        "os.makedirs(data_path + '/SerializedData/', exist_ok=True)  # Create SerializedData directory\n",
        "\n",
        "# Path to the serialized data stored in the drive\n",
        "serialized_file_path = '/content/drive/MyDrive/ML final project/SerializedData/'\n",
        "\n",
        "\n",
        "# Mounts drive to google drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Global definition for Cascade Classifier from google drive\n",
        "eye_cascade = cv2.CascadeClassifier(data_path + '/code/haarcascade_eye.xml')\n",
        "\n",
        "imgs_paths = []  # List to store file paths for all images\n",
        "eye_detected_imgs_paths = []  # List to store file paths for images with detected eyes\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eodKykXlTB-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "174b7aaa-588b-48c9-8d96-b853aa2cab6a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-processing\n",
        "\n",
        "The first block of code will load the previously saved serialized data from the shared drive.\n",
        "\n",
        "The second block is the code used to create the serialized data from the dataset's images."
      ],
      "metadata": {
        "id": "rReW8htJrf_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load All Previously Serialized Data"
      ],
      "metadata": {
        "id": "c0_DpHT_rqCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Run this if you are starting a new session to load all of the serialized data\n",
        "\n",
        "# Load the file paths for imgs\n",
        "with open(os.path.join(data_path, 'SerializedData', 'imgs_paths.pkl'), 'rb') as f:\n",
        "    imgs_paths = pickle.load(f)\n",
        "\n",
        "# Load the file paths for eye detected images\n",
        "with open(os.path.join(data_path, 'SerializedData', 'eye_detected_imgs_paths.pkl'), 'rb') as f:\n",
        "    eye_detected_imgs_paths = pickle.load(f)\n",
        "\n",
        "with open(serialized_file_path + 'processed_image_data.pkl', 'rb') as f:\n",
        "    loaded_data = pickle.load(f)\n",
        "\n",
        "# Access the loaded variables\n",
        "iris_eye_detected_imgs = loaded_data['iris_eye_detected_imgs']\n",
        "images = loaded_data['images']\n",
        "iris_num = loaded_data['iris_num']\n",
        "total_images_processed = loaded_data['total_images_processed']\n",
        "\n",
        "\n",
        "print(\"File paths loaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKB75zJyray2",
        "outputId": "b61743ea-5930-4d6d-ad1f-101988a325f4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File paths loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Does not need to be run each session\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "\n",
        "# Define data path and ensure cascade directory exists\n",
        "os.makedirs(data_path + '/eye_cascade/', exist_ok=True)\n",
        "\n",
        "# Function to apply thresholding and morphological transformations\n",
        "def transform_image(img, threshold):\n",
        "    if threshold == 0:\n",
        "        _, threshold = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    else:\n",
        "        retval, threshold = cv2.threshold(img, threshold, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Apply morphological operations\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
        "    opening = cv2.morphologyEx(threshold, cv2.MORPH_OPEN, kernel)\n",
        "    closing = cv2.morphologyEx(threshold, cv2.MORPH_CLOSE, kernel)\n",
        "    open_close = cv2.bitwise_or(opening, closing, mask=None)\n",
        "\n",
        "    return open_close, opening, closing\n",
        "\n",
        "# Initialize lists for images and labels\n",
        "imgs = []\n",
        "label = 0\n",
        "path = \"/content/drive/MyDrive/ML final project/datasets/CLASSES_400_300_Part2\"\n",
        "\n",
        "for filepath in glob.iglob(path + '/**', recursive=True):\n",
        "    num_in_folder = 0\n",
        "    for filefilepath in glob.iglob(filepath + '/**/*.tiff', recursive=True):\n",
        "        # Store the file path, folder number, label\n",
        "        img_colored = cv2.imread(filefilepath)\n",
        "        img_gray = cv2.cvtColor(cv2.resize(img_colored, (200, 150)), cv2.COLOR_BGR2GRAY)\n",
        "        imgs_paths.append([filefilepath, num_in_folder, label])  # Save the file path and metadata\n",
        "        num_in_folder += 1\n",
        "    label += 1\n",
        "\n",
        "# Eye detection and processing\n",
        "eyes_num = 0\n",
        "for img_path, j, L in imgs_paths:\n",
        "    # Read and process the image\n",
        "    img = cv2.imread(img_path)\n",
        "    img_gray = cv2.cvtColor(cv2.resize(img, (400, 400)), cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect eyes in the image using the eye cascade classifier\n",
        "    eyes = eye_cascade.detectMultiScale(img_gray, scaleFactor=1.1, minNeighbors=3)\n",
        "\n",
        "    if len(eyes) > 1:\n",
        "        print(eyes_num)\n",
        "        eye_detected_imgs_paths.append([img_path, j, L])  # Save the file path for detected images\n",
        "        eyes_num += 1\n",
        "\n",
        "        maxium_area = -3\n",
        "        for (ex, ey, ew, eh) in eyes:\n",
        "            area = ew * eh\n",
        "            if area > maxium_area:\n",
        "                maxium_area = area\n",
        "                maxium_width = ew\n",
        "                point_x = ex\n",
        "                point_y = ey\n",
        "                maxium_height = eh\n",
        "\n",
        "        # Optionally, draw rectangle around largest eye (currently commented out)\n",
        "        # cv2.rectangle(img, (point_x, point_y), (point_x + maxium_width, point_y + maxium_height), (255, 0, 0), 2)\n",
        "\n",
        "# Print summary\n",
        "print(\"Total eyes found: \", eyes_num)\n",
        "print(\"Total images processed: \", len(imgs_paths))\n",
        "\n",
        "# Save the file paths for imgs and eye_detected_imgs to disk\n",
        "with open(os.path.join(data_path, 'SerializedData', 'imgs_paths.pkl'), 'wb') as f:\n",
        "    pickle.dump(imgs_paths, f)\n",
        "\n",
        "with open(os.path.join(data_path, 'SerializedData', 'eye_detected_imgs_paths.pkl'), 'wb') as f:\n",
        "    pickle.dump(eye_detected_imgs_paths, f)\n",
        "\n",
        "print(\"File paths saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pakqe6xuTO4j",
        "outputId": "0dfc227a-02cb-4d7f-f2d5-42befbe57296"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "Total eyes found:  714\n",
            "Total images processed:  2940\n",
            "File paths saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eDWBgQz4wy1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Initialize a counter to track the number of iris images found\n",
        "iris_num = 0\n",
        "iris_eye_detected_imgs = []\n",
        "\n",
        "def is_valid_circle(x, y, r, img_shape, min_radius=20):\n",
        "    \"\"\"Check if the circle is within the image bounds and meets the minimum radius condition.\"\"\"\n",
        "    return (x + r <= img_shape[1] and y + r <= img_shape[0] and\n",
        "            x - r > 0 and y - r > 0 and r > min_radius)\n",
        "\n",
        "# Initialize an array to store the images\n",
        "images = []\n",
        "\n",
        "# Load all images into the array from the file paths\n",
        "for img_path, j, L in eye_detected_imgs_paths:\n",
        "\n",
        "    # Load the image in color for processing (or grayscale as required)\n",
        "    c = cv2.imread(img_path)  # Load the color image (original)\n",
        "\n",
        "    # Load the image in grayscale for Hough Circle detection\n",
        "    i = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Check if the image was loaded correctly\n",
        "    if i is not None:\n",
        "        images.append((i, img_path, j, L, c))  # Store the image array, metadata, and color image\n",
        "    else:\n",
        "        print(f\"Error loading image: {img_path}\")\n",
        "\n",
        "# Loop through each loaded image for processing\n",
        "for i, img_path, j, L, c in images:\n",
        "    # Apply the Hough Circle Transform to detect circles in the grayscale image 'i'\n",
        "    circles = cv2.HoughCircles(i, cv2.HOUGH_GRADIENT, dp=1.2, minDist=100, param1=50, param2=30, minRadius=20, maxRadius=100)\n",
        "\n",
        "    if circles is not None:\n",
        "        # Round the coordinates and radii of the detected circles to integers\n",
        "        circles = np.round(circles[0, :]).astype(\"int\")\n",
        "\n",
        "        # Initialize a high maximum average intensity for comparison\n",
        "        maximum_average = float('inf')\n",
        "        best_circle = None  # To store the best circle\n",
        "\n",
        "        # Loop through each detected circle and find the one with the lowest average intensity\n",
        "        for (x, y, r) in circles:\n",
        "            if is_valid_circle(x, y, r, i.shape):\n",
        "                # Define a Region of Interest (ROI) around the detected circle\n",
        "                new_roi = i[y - r:y + r, x - r:x + r]\n",
        "                # Calculate the average pixel intensity of the ROI\n",
        "                average = np.average(new_roi)\n",
        "\n",
        "                # Update if this circle has the lowest average intensity found so far\n",
        "                if average < maximum_average:\n",
        "                    maximum_average = average\n",
        "                    best_circle = (x, y, r)\n",
        "\n",
        "        # If no valid circle found, select the circle with the largest radius\n",
        "        if best_circle is None:\n",
        "            best_circle = max(circles, key=lambda circle: circle[2])\n",
        "\n",
        "        # Optionally, draw the detected iris circle on the original color image `c` (optional)\n",
        "        # cv2.circle(c, (best_circle[0], best_circle[1]), best_circle[2], (255, 255, 0), 4)\n",
        "\n",
        "        # Save the annotated image to the specified directory on Google Drive with a unique name\n",
        "        output_filename = f\"/content/drive/MyDrive/ML final project/datasets/iris/{L}.{j}.jpg\"\n",
        "        cv2.imwrite(output_filename, c)\n",
        "\n",
        "        # Add the detected iris image to the iris_eye_detected_imgs list\n",
        "        iris_eye_detected_imgs.append((img_path, j, L, c))\n",
        "\n",
        "        # Increment the iris count\n",
        "        iris_num += 1\n",
        "\n",
        "# Print the total number of iris images found\n",
        "print(\"total_iris_found =\", iris_num)\n",
        "\n",
        "# Print the total number of images processed\n",
        "print(\"total images number\", len(images))\n",
        "\n",
        "\n",
        "# Variables to serialize\n",
        "data_to_serialize = {\n",
        "    'iris_eye_detected_imgs': iris_eye_detected_imgs,\n",
        "    'images': images,\n",
        "    'iris_num': iris_num,\n",
        "    'total_images_processed': len(images),\n",
        "}\n",
        "\n",
        "# Open the file in write-binary mode and serialize the variables\n",
        "with open(serialized_file_path + 'processed_image_data.pkl', 'wb') as f:\n",
        "    pickle.dump(data_to_serialize, f)\n",
        "\n",
        "print(f\"Serialized data has been saved to {serialized_file_path + 'processed_image_data.pkl'}\")\n"
      ],
      "metadata": {
        "id": "OvxC8Nj3TYva",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "c7b140a6-a6c4-4ad6-e546-f409a75ce021"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_iris_found = 714\n",
            "total images number 714\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IsADirectoryError",
          "evalue": "[Errno 21] Is a directory: '/content/drive/MyDrive/ML final project/SerializedData/'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a596db7606f1>\u001b[0m in \u001b[0;36m<cell line: 91>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# Open the file in write-binary mode and serialize the variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_serialize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/content/drive/MyDrive/ML final project/SerializedData/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the file in write-binary mode and serialize the variables\n",
        "with open(serialized_file_path + 'processed_image_data.pkl', 'wb') as f:\n",
        "    pickle.dump(data_to_serialize, f)\n",
        "\n",
        "print(f\"Serialized data has been saved to {serialized_file_path}\")\n"
      ],
      "metadata": {
        "id": "bxHNCzyZ1X9K",
        "outputId": "52cfeb7b-21dc-4b16-eb89-bafbaf392849",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serialized data has been saved to /content/drive/MyDrive/ML final project/SerializedData/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH1vmA-NUc1h"
      },
      "outputs": [],
      "source": [
        "# Copy the list of detected iris images to 'imgs' and shuffle them randomly\n",
        "imgs = iris_eye_detected_imgs\n",
        "\n",
        "# Define a 5x5 kernel for image transformations (e.g., dilation or erosion)\n",
        "kernel = np.ones((5,5), np.uint8)\n",
        "\n",
        "# Import the random module and shuffle 'imgs' to randomize the processing order\n",
        "import random\n",
        "random.shuffle(imgs)\n",
        "\n",
        "# Initialize lists to store the final output, labels, and test images\n",
        "test = []\n",
        "final_output = []\n",
        "lables = []\n",
        "\n",
        "# Loop through each image and associated metadata in 'imgs'\n",
        "for i, j, L, c in imgs:\n",
        "    # Perform a transformation on the image 'i' with an initial threshold of 0\n",
        "    # 'gold', 'siver', and 'diamond' represent different transformations of the image\n",
        "    gold, siver, diamond = transform_image(i, 0)\n",
        "    golden_refrence = sum(sum(gold))  # Sum the pixel values of the 'gold' transformed image\n",
        "\n",
        "    # Loop through threshold values from 10 to 1000 in increments of 10\n",
        "    for k in range(10, 1000, 10):\n",
        "        # Apply transformations with the current threshold 'k'\n",
        "        working_img, opening, closing = transform_image(i, k)\n",
        "        suming = sum(sum(working_img))  # Sum the pixel values of the 'working_img'\n",
        "        diffrence = suming - golden_refrence  # Calculate the difference from the golden reference\n",
        "\n",
        "        # If the difference in pixel values is significant (threshold found)\n",
        "        if diffrence > 800:\n",
        "            print(\"The image threshold =\", k)\n",
        "            print(\"The image name\", j)\n",
        "            print(\" \")\n",
        "\n",
        "            # Save images with applied transformations for different thresholds\n",
        "            cv2.imwrite(\"/content/drive/MyDrive/ML final project/datasets/threshold/\" + str(L) + '.' + str(j) + '.jpg', working_img)\n",
        "            cv2.imwrite(\"/content/drive/MyDrive/ML final project/datasets/opening/\" + str(L) + '.' + str(j) + '.jpg', opening)\n",
        "            cv2.imwrite(\"/content/drive/MyDrive/ML final project/datasets/closing/\" + str(L) + '.' + str(j) + '.jpg', closing)\n",
        "\n",
        "            # Find contours in the thresholded image 'working_img'\n",
        "            contours, _ = cv2.findContours(working_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "            # Draw bounding rectangles around the detected contours if they meet specific criteria\n",
        "            for z in contours:\n",
        "                x, y, w, h = cv2.boundingRect(z)\n",
        "                if x + w < 150 and y + h < 200 and x - w // 4 > 0:\n",
        "                    cv2.rectangle(working_img, (x, y), (x + w, y + h), (0, 255, 0), -2)\n",
        "                    cv2.imwrite(\"/content/drive/MyDrive/ML final project/datasets/contour/\" + str(L) + '.' + str(j) + '.jpg', working_img)\n",
        "\n",
        "            # Find contours again for further processing\n",
        "            contours_2, _ = cv2.findContours(working_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "            # Initialize variables to store properties of the largest contour found\n",
        "            maxium_area = 0\n",
        "            maxium_width = 0\n",
        "            point_x = 0\n",
        "            point_y = 0\n",
        "            maxium_height = 0\n",
        "\n",
        "            # Loop through contours to find the largest one by area\n",
        "            for z in contours_2:\n",
        "                x, y, w, h = cv2.boundingRect(z)\n",
        "                new_area = h * w\n",
        "                if x + w < 150 and y + h < 200 and new_area > maxium_area and x - w // 4 > 0:\n",
        "                    maxium_area = new_area\n",
        "                    maxium_width = w\n",
        "                    point_x = x\n",
        "                    point_y = y\n",
        "                    maxium_height = h\n",
        "\n",
        "            # Define the center of the largest bounding box and set a radius for cropping the region of interest (ROI)\n",
        "            center_x = point_x + maxium_width // 2\n",
        "            center_y = point_y + maxium_height // 2\n",
        "            radius = 40\n",
        "\n",
        "            # Ensure the cropping region is within bounds and save the cropped ROI\n",
        "            if center_y - radius > 0 and center_x - radius > 0 and center_y + radius < 200 and center_x + radius < 150:\n",
        "                new_roi = c[center_y - radius:center_y + radius, center_x - radius:center_x + radius]\n",
        "                new_roi = cv2.resize(new_roi, (200, 150))\n",
        "                cv2.imwrite(\"/content/drive/MyDrive/ML final project/datasets/final_iris/\" + str(L) + '.' + str(j) + '.jpg', new_roi)\n",
        "            else:\n",
        "                # Default to using the center of the image if the region goes out of bounds\n",
        "                center_y = c.shape[0] // 2\n",
        "                center_x = c.shape[1] // 2\n",
        "                new_roi = c[center_y - radius:center_y + radius, center_x - radius:center_x + radius]\n",
        "                new_roi = cv2.resize(new_roi, (200, 150))\n",
        "                cv2.imwrite(\"/content/drive/MyDrive/ML final project/datasets/final_iris/\" + str(L) + '.' + str(j) + '.jpg', new_roi)\n",
        "\n",
        "            # Save the transformed original image and add data to the lists\n",
        "            cv2.imwrite(\"/content/drive/MyDrive/ML final project/datasets/edging_5/\" + str(L) + '_' + str(j) + '.jpg', i)\n",
        "            test.append(i)\n",
        "            final_output.append(new_roi)\n",
        "            lables.append(L)\n",
        "\n",
        "            # Break out of the threshold loop once a suitable threshold is found\n",
        "            break\n",
        "\n",
        "# Display the number of final images and labels generated\n",
        "print(\"The length of final output =\", len(final_output))\n",
        "print(\"The number of labels =\", len(lables))\n",
        "\n",
        "# Convert lists to numpy arrays for easier storage and access\n",
        "final_output = np.array(final_output)\n",
        "print(final_output.shape)\n",
        "\n",
        "test = np.array(test)\n",
        "print(test.shape)\n",
        "\n",
        "# Save the processed data and labels using pickle for later use\n",
        "import pickle\n",
        "\n",
        "pickle_out = open(\"test_ubiris.pickle\", \"wb\")\n",
        "pickle.dump(test, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "pickle_out = open(\"ubiris_features.pickle\", \"wb\")\n",
        "pickle.dump(final_output, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "pickle_out = open(\"ubiris_lables.pickle\", \"wb\")\n",
        "pickle.dump(lables, pickle_out)\n",
        "pickle_out.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2MpPzJnhirAz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}