{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "import pickle\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import drive\n",
        "\n",
        "# Initialize the path to the shared folder globally\n",
        "data_path = '/content/drive/MyDrive/ML final project'\n",
        "os.makedirs(data_path + '/SerializedData/', exist_ok=True)  # Create SerializedData directory\n",
        "\n",
        "# Mounts drive to google drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Global definition for Cascade Classifier from google drive\n",
        "eye_cascade = cv2.CascadeClassifier(data_path + '/code/haarcascade_eye.xml')\n",
        "\n",
        "imgs_paths = []  # List to store file paths for all images\n",
        "eye_detected_imgs_paths = []  # List to store file paths for images with detected eyes\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eodKykXlTB-N",
        "outputId": "063673e4-8521-4d50-d720-cb1d95eb17cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-processing\n",
        "\n",
        "The first block of code will load the previously saved serialized data from the shared drive.\n",
        "\n",
        "The second block is the code used to create the serialized data from the dataset's images."
      ],
      "metadata": {
        "id": "rReW8htJrf_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SerializedData - Images"
      ],
      "metadata": {
        "id": "c0_DpHT_rqCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Run this if you are starting a new session to load all of the serialized data\n",
        "\n",
        "# Load the file paths for imgs\n",
        "with open(os.path.join(data_path, 'SerializedData', 'imgs_paths.pkl'), 'rb') as f:\n",
        "    imgs_paths = pickle.load(f)\n",
        "\n",
        "# Load the file paths for eye detected images\n",
        "with open(os.path.join(data_path, 'SerializedData', 'eye_detected_imgs_paths.pkl'), 'rb') as f:\n",
        "    eye_detected_imgs_paths = pickle.load(f)\n",
        "\n",
        "print(\"File paths loaded successfully.\")"
      ],
      "metadata": {
        "id": "fKB75zJyray2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Does not need to be run each session\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "\n",
        "# Define data path and ensure cascade directory exists\n",
        "os.makedirs(data_path + '/eye_cascade/', exist_ok=True)\n",
        "\n",
        "# Function to apply thresholding and morphological transformations\n",
        "def transform_image(img, threshold):\n",
        "    if threshold == 0:\n",
        "        _, threshold = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    else:\n",
        "        retval, threshold = cv2.threshold(img, threshold, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Apply morphological operations\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
        "    opening = cv2.morphologyEx(threshold, cv2.MORPH_OPEN, kernel)\n",
        "    closing = cv2.morphologyEx(threshold, cv2.MORPH_CLOSE, kernel)\n",
        "    open_close = cv2.bitwise_or(opening, closing, mask=None)\n",
        "\n",
        "    return open_close, opening, closing\n",
        "\n",
        "# Initialize lists for images and labels\n",
        "imgs = []\n",
        "label = 0\n",
        "path = \"/content/drive/MyDrive/ML final project/datasets/CLASSES_400_300_Part2\"\n",
        "\n",
        "for filepath in glob.iglob(path + '/**', recursive=True):\n",
        "    num_in_folder = 0\n",
        "    for filefilepath in glob.iglob(filepath + '/**/*.tiff', recursive=True):\n",
        "        # Store the file path, folder number, label\n",
        "        img_colored = cv2.imread(filefilepath)\n",
        "        img_gray = cv2.cvtColor(cv2.resize(img_colored, (200, 150)), cv2.COLOR_BGR2GRAY)\n",
        "        imgs_paths.append([filefilepath, num_in_folder, label])  # Save the file path and metadata\n",
        "        num_in_folder += 1\n",
        "    label += 1\n",
        "\n",
        "# Eye detection and processing\n",
        "eyes_num = 0\n",
        "for img_path, j, L in imgs_paths:\n",
        "    # Read and process the image\n",
        "    img = cv2.imread(img_path)\n",
        "    img_gray = cv2.cvtColor(cv2.resize(img, (400, 400)), cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect eyes in the image using the eye cascade classifier\n",
        "    eyes = eye_cascade.detectMultiScale(img_gray, scaleFactor=1.1, minNeighbors=3)\n",
        "\n",
        "    if len(eyes) > 1:\n",
        "        print(eyes_num)\n",
        "        eye_detected_imgs_paths.append([img_path, j, L])  # Save the file path for detected images\n",
        "        eyes_num += 1\n",
        "\n",
        "        maxium_area = -3\n",
        "        for (ex, ey, ew, eh) in eyes:\n",
        "            area = ew * eh\n",
        "            if area > maxium_area:\n",
        "                maxium_area = area\n",
        "                maxium_width = ew\n",
        "                point_x = ex\n",
        "                point_y = ey\n",
        "                maxium_height = eh\n",
        "\n",
        "        # Optionally, draw rectangle around largest eye (currently commented out)\n",
        "        # cv2.rectangle(img, (point_x, point_y), (point_x + maxium_width, point_y + maxium_height), (255, 0, 0), 2)\n",
        "\n",
        "# Print summary\n",
        "print(\"Total eyes found: \", eyes_num)\n",
        "print(\"Total images processed: \", len(imgs_paths))\n",
        "\n",
        "# Save the file paths for imgs and eye_detected_imgs to disk\n",
        "with open(os.path.join(data_path, 'SerializedData', 'imgs_paths.pkl'), 'wb') as f:\n",
        "    pickle.dump(imgs_paths, f)\n",
        "\n",
        "with open(os.path.join(data_path, 'SerializedData', 'eye_detected_imgs_paths.pkl'), 'wb') as f:\n",
        "    pickle.dump(eye_detected_imgs_paths, f)\n",
        "\n",
        "print(\"File paths saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pakqe6xuTO4j",
        "outputId": "2e1b7fc1-201a-4190-e4f8-af3f6c0c4007"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total eyes found:  0\n",
            "Total images processed:  0\n",
            "File paths saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a counter to track the number of iris images found\n",
        "iris_num = 0\n",
        "\n",
        "# Loop through each image and its associated metadata in the list of eye-detected images\n",
        "for i, j, L, c in eye_detected_imgs:\n",
        "\n",
        "    # Apply the Hough Circle Transform to detect circles in the grayscale image 'i'\n",
        "    circles = cv2.HoughCircles(i, cv2.HOUGH_GRADIENT, 10, 100)\n",
        "\n",
        "    # If circles were detected\n",
        "    if circles is not None:\n",
        "        # Round the coordinates and radii of the detected circles to integers\n",
        "        circles = np.round(circles[0, :]).astype(\"int\")\n",
        "\n",
        "        # Initialize a very high maximum average intensity for comparison\n",
        "        maxiumum_average = 10000000000000\n",
        "        key = True  # A flag to indicate if no valid circles were found within the criteria\n",
        "\n",
        "        # Loop through each detected circle\n",
        "        for (x, y, r) in circles:\n",
        "            # Check if the circle is entirely within the image boundaries and has a minimum radius\n",
        "            if x + r <= max(i.shape) and y + r <= max(i.shape) and x - r > 0 and y - r > 0 and r > 20:\n",
        "                key = False  # Set the flag to False since a valid circle was found\n",
        "\n",
        "                # Define a Region of Interest (ROI) around the detected circle\n",
        "                new_roi = i[y - r:y + r, x - r:x + r]\n",
        "                # Calculate the average pixel intensity of the ROI\n",
        "                average = np.average(new_roi)\n",
        "\n",
        "                # Update if this circle has the lowest average intensity found so far\n",
        "                if average < maxiumum_average:\n",
        "                    maxiumum_r = r\n",
        "                    point_x = x\n",
        "                    point_y = y\n",
        "                    maxiumum_average = average\n",
        "\n",
        "        # If no circle met the criteria (i.e., key is still True)\n",
        "        if key:\n",
        "            # Set the average intensity to infinity to ensure this block is only for fallback cases\n",
        "            average = float('inf')\n",
        "            maxiumu_raduis = -4  # Initialize a variable to store the maximum radius found\n",
        "\n",
        "            # Loop again through the circles as a fallback, choosing the largest radius\n",
        "            for (x, y, r) in circles:\n",
        "                if r > maxiumu_raduis:\n",
        "                    maxiumum_r = r\n",
        "                    point_x = x\n",
        "                    point_y = y\n",
        "                    maxiumum_average = average\n",
        "\n",
        "        # Optionally, draw the detected iris circle on the original color image `c` (currently commented out)\n",
        "        # cv2.circle(c, (point_x, point_y), maxiumum_r, (255, 255, 0), 4)\n",
        "\n",
        "        # Save the annotated image to the specified directory on Google Drive with a unique name based on `L` and `j`\n",
        "        cv2.imwrite(\"/content/drive/MyDrive/ML final project/datasets/iris/\" + str(L) + '.' + str(j) + '.jpg', c)\n",
        "\n",
        "        # Add the detected iris image to the iris_eye_detected_imgs list\n",
        "        iris_eye_detected_imgs.append(eye_detected_imgs[iris_num])\n",
        "        # Increment the iris count\n",
        "        iris_num += 1\n",
        "\n",
        "# Print the total number of iris images found\n",
        "print(\"total_iris_found = \", iris_num)\n",
        "\n",
        "# Print the total number of images processed\n",
        "print(\"total images number \", len(imgs))\n"
      ],
      "metadata": {
        "id": "OvxC8Nj3TYva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH1vmA-NUc1h"
      },
      "outputs": [],
      "source": [
        "# Copy the list of detected iris images to 'imgs' and shuffle them randomly\n",
        "imgs = iris_eye_detected_imgs\n",
        "\n",
        "# Define a 5x5 kernel for image transformations (e.g., dilation or erosion)\n",
        "kernel = np.ones((5,5), np.uint8)\n",
        "\n",
        "# Import the random module and shuffle 'imgs' to randomize the processing order\n",
        "import random\n",
        "random.shuffle(imgs)\n",
        "\n",
        "# Initialize lists to store the final output, labels, and test images\n",
        "test = []\n",
        "final_output = []\n",
        "lables = []\n",
        "\n",
        "# Loop through each image and associated metadata in 'imgs'\n",
        "for i, j, L, c in imgs:\n",
        "    # Perform a transformation on the image 'i' with an initial threshold of 0\n",
        "    # 'gold', 'siver', and 'diamond' represent different transformations of the image\n",
        "    gold, siver, diamond = transform_image(i, 0)\n",
        "    golden_refrence = sum(sum(gold))  # Sum the pixel values of the 'gold' transformed image\n",
        "\n",
        "    # Loop through threshold values from 10 to 1000 in increments of 10\n",
        "    for k in range(10, 1000, 10):\n",
        "        # Apply transformations with the current threshold 'k'\n",
        "        working_img, opening, closing = transform_image(i, k)\n",
        "        suming = sum(sum(working_img))  # Sum the pixel values of the 'working_img'\n",
        "        diffrence = suming - golden_refrence  # Calculate the difference from the golden reference\n",
        "\n",
        "        # If the difference in pixel values is significant (threshold found)\n",
        "        if diffrence > 800:\n",
        "            print(\"The image threshold =\", k)\n",
        "            print(\"The image name\", j)\n",
        "            print(\" \")\n",
        "\n",
        "            # Save images with applied transformations for different thresholds\n",
        "            cv2.imwrite(\"/content/drive/MyDrive/ML final project/datasets/threshold/\" + str(L) + '.' + str(j) + '.jpg', working_img)\n",
        "            cv2.imwrite(\"/content/drive/MyDrive/ML final project/datasets/opening/\" + str(L) + '.' + str(j) + '.jpg', opening)\n",
        "            cv2.imwrite(\"/content/drive/MyDrive/ML final project/datasets/closing/\" + str(L) + '.' + str(j) + '.jpg', closing)\n",
        "\n",
        "            # Find contours in the thresholded image 'working_img'\n",
        "            contours, _ = cv2.findContours(working_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "            # Draw bounding rectangles around the detected contours if they meet specific criteria\n",
        "            for z in contours:\n",
        "                x, y, w, h = cv2.boundingRect(z)\n",
        "                if x + w < 150 and y + h < 200 and x - w // 4 > 0:\n",
        "                    cv2.rectangle(working_img, (x, y), (x + w, y + h), (0, 255, 0), -2)\n",
        "                    cv2.imwrite(\"/content/drive/MyDrive/ML final project/datasets/contour/\" + str(L) + '.' + str(j) + '.jpg', working_img)\n",
        "\n",
        "            # Find contours again for further processing\n",
        "            contours_2, _ = cv2.findContours(working_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "            # Initialize variables to store properties of the largest contour found\n",
        "            maxium_area = 0\n",
        "            maxium_width = 0\n",
        "            point_x = 0\n",
        "            point_y = 0\n",
        "            maxium_height = 0\n",
        "\n",
        "            # Loop through contours to find the largest one by area\n",
        "            for z in contours_2:\n",
        "                x, y, w, h = cv2.boundingRect(z)\n",
        "                new_area = h * w\n",
        "                if x + w < 150 and y + h < 200 and new_area > maxium_area and x - w // 4 > 0:\n",
        "                    maxium_area = new_area\n",
        "                    maxium_width = w\n",
        "                    point_x = x\n",
        "                    point_y = y\n",
        "                    maxium_height = h\n",
        "\n",
        "            # Define the center of the largest bounding box and set a radius for cropping the region of interest (ROI)\n",
        "            center_x = point_x + maxium_width // 2\n",
        "            center_y = point_y + maxium_height // 2\n",
        "            radius = 40\n",
        "\n",
        "            # Ensure the cropping region is within bounds and save the cropped ROI\n",
        "            if center_y - radius > 0 and center_x - radius > 0 and center_y + radius < 200 and center_x + radius < 150:\n",
        "                new_roi = c[center_y - radius:center_y + radius, center_x - radius:center_x + radius]\n",
        "                new_roi = cv2.resize(new_roi, (200, 150))\n",
        "                cv2.imwrite(\"/content/drive/MyDrive/ML final project/datasets/final_iris/\" + str(L) + '.' + str(j) + '.jpg', new_roi)\n",
        "            else:\n",
        "                # Default to using the center of the image if the region goes out of bounds\n",
        "                center_y = c.shape[0] // 2\n",
        "                center_x = c.shape[1] // 2\n",
        "                new_roi = c[center_y - radius:center_y + radius, center_x - radius:center_x + radius]\n",
        "                new_roi = cv2.resize(new_roi, (200, 150))\n",
        "                cv2.imwrite(\"/content/drive/MyDrive/ML final project/datasets/final_iris/\" + str(L) + '.' + str(j) + '.jpg', new_roi)\n",
        "\n",
        "            # Save the transformed original image and add data to the lists\n",
        "            cv2.imwrite(\"/content/drive/MyDrive/ML final project/datasets/edging_5/\" + str(L) + '_' + str(j) + '.jpg', i)\n",
        "            test.append(i)\n",
        "            final_output.append(new_roi)\n",
        "            lables.append(L)\n",
        "\n",
        "            # Break out of the threshold loop once a suitable threshold is found\n",
        "            break\n",
        "\n",
        "# Display the number of final images and labels generated\n",
        "print(\"The length of final output =\", len(final_output))\n",
        "print(\"The number of labels =\", len(lables))\n",
        "\n",
        "# Convert lists to numpy arrays for easier storage and access\n",
        "final_output = np.array(final_output)\n",
        "print(final_output.shape)\n",
        "\n",
        "test = np.array(test)\n",
        "print(test.shape)\n",
        "\n",
        "# Save the processed data and labels using pickle for later use\n",
        "import pickle\n",
        "\n",
        "pickle_out = open(\"test_ubiris.pickle\", \"wb\")\n",
        "pickle.dump(test, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "pickle_out = open(\"ubiris_features.pickle\", \"wb\")\n",
        "pickle.dump(final_output, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "pickle_out = open(\"ubiris_lables.pickle\", \"wb\")\n",
        "pickle.dump(lables, pickle_out)\n",
        "pickle_out.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2MpPzJnhirAz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}