{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2RD3orSFBQJ"
      },
      "source": [
        "# Final notebook with experiments for eye reconstruction and manipulation.\n",
        "\n",
        "After training a decent autencoder over the ssh server, we began experimenting with different methods of changing the color of the eye, and manipulating the latent vectors or embeddings of te images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_n7hFoYxG-aW",
        "outputId": "79363785-1c58-42bf-a669-fc9342bcc841"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# USE THOS\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAZcxmW6yY3I"
      },
      "source": [
        "# Necessary imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJEMkIaRtE0p"
      },
      "source": [
        "Install commands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVFnnIz1tSvD"
      },
      "source": [
        "The line where the decoder saved state is loaded (decoder = load_model(filepath, custom_objects={'Conv2DTranspose': tf.keras.layers.Conv2DTranspose})) will not work unless the tensorflow version is 2.11.0.\n",
        "\n",
        "Once ran, you will need to restart the session. You will not need to run the cell to install tensorflow v2.11.0 again. You may need to install the other libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QwKUmkoUtLty"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0PEg_aKKtG3m",
        "outputId": "1df521a3-286a-4cfb-e0ec-d7f0fee057c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install Pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZgodfnPNtPFi",
        "outputId": "c3ccc8e4-b176-4841-c36c-afc42a782d0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: colormath in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colormath) (1.26.4)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.10/dist-packages (from colormath) (3.4.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install colormath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kfTWkd-hZRVE",
        "outputId": "f13dd3a8-f5c9-45d3-8f24-ae745aece563"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.8.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.5.1 (from gradio)\n",
            "  Downloading gradio_client-1.5.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.19-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.8.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.1->gradio) (2024.10.0)\n",
            "Collecting websockets<15.0,>=10.0 (from gradio-client==1.5.1->gradio)\n",
            "  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.8.0-py3-none-any.whl (57.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.1-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.2/320.2 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading python_multipart-0.0.19-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.8.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.6 ffmpy-0.4.0 gradio-5.8.0 gradio-client-1.5.1 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.19 ruff-0.8.3 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.13.2 uvicorn-0.32.1 websockets-14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8pVS9_O8ZqcA",
        "outputId": "e1d8be08-e113-4684-cf53-20720d28d97e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.36.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio) (1.26.4)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (11.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sIZajXqVKD0i",
        "outputId": "4ec07fee-f214-4999-da09-e189e075074e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting umap-learn\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.5.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.60.0)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.6)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
            "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.13 umap-learn-0.5.7\n"
          ]
        }
      ],
      "source": [
        "!pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Reshape, Conv2DTranspose, BatchNormalization\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from scipy import interpolate\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import imageio\n",
        "import umap.umap_ as umap\n",
        "from sklearn.cluster import KMeans\n",
        "from colormath.color_objects import sRGBColor, LabColor\n",
        "from colormath.color_conversions import convert_color\n",
        "from colormath.color_diff import delta_e_cie2000\n",
        "\n",
        "# USE THOS"
      ],
      "metadata": {
        "id": "vRcj4jyeToBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1dTCIV2yalU"
      },
      "source": [
        "# Loading Encoder/Data\n",
        "\n",
        "This loads the saved state of the first half of the autencoder, which is the encoder. Over the remote server, the UBIRIS images are passed into the encoder, and the result is data and vectors representing each image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T502qJnP5Omc",
        "outputId": "b07e5fe1-4ec0-4932-adfc-6d52e2beaa6f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Load the saved latent vectors and encoder model (FULL)\\nfilepath = r'/content/drive/MyDrive/ML final project/Trained_models/encoder/encoder_model1.5.h5'\\nlatent_vectors = np.load('/content/drive/MyDrive/ML final project/Trained_models/encoder/latent_vectors1.5.npy')\\nimage_data = np.load('/content/drive/MyDrive/ML final project/Trained_models/encoder/image_data1.5.npy')\\nencoder = load_model(filepath)\\n#encoder = tf.keras.layers.TFSMLayer('encoder_model', call_endpoint='serving_default')\\n#image_data = np.load('/content/drive/MyDrive/ML final project/Trained_models/encoder/encoder_filtered/image_data8.npy')\\n\""
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# USE THOS\n",
        "\n",
        "IMG_SIZE = 128\n",
        "LATENT_DIM = 32\n",
        "\n",
        "\n",
        "image1 = cv2.imread('/content/drive/MyDrive/ML final project/our_eyes/IMG_1636.JPG')\n",
        "image2 = cv2.imread('/content/drive/MyDrive/ML final project/our_eyes/IMG_1642.JPG')\n",
        "image3 = cv2.imread('/content/drive/MyDrive/ML final project/our_eyes/IMG_1656.JPG')\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# Load the saved latent vectors and encoder model (FULL)\n",
        "filepath = r'/content/drive/MyDrive/ML final project/Trained_models/encoder/encoder_model1.5.h5'\n",
        "latent_vectors = np.load('/content/drive/MyDrive/ML final project/Trained_models/encoder/latent_vectors1.5.npy')\n",
        "image_data = np.load('/content/drive/MyDrive/ML final project/Trained_models/encoder/image_data1.5.npy')\n",
        "encoder = load_model(filepath)\n",
        "#encoder = tf.keras.layers.TFSMLayer('encoder_model', call_endpoint='serving_default')\n",
        "#image_data = np.load('/content/drive/MyDrive/ML final project/Trained_models/encoder/encoder_filtered/image_data8.npy')\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d48VQcajJdZr",
        "outputId": "ed901ad5-2ac3-47c1-fd23-efa081913636"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ],
      "source": [
        "# Paths\n",
        "TEST_IMAGE_FOLDER = '/content/drive/MyDrive/ML final project/datasets/testData'\n",
        "TEST_LABELS_CSV = '/content/drive/MyDrive/ML final project/datasets/test_labels.csv'\n",
        "filepath_encoder = '/content/drive/MyDrive/ML final project/Trained_models/encoder/encoder_model_f32'\n",
        "filepath_decoder = '/content/drive/MyDrive/ML final project/Trained_models/decoder/decoder_model_f32'\n",
        "filepath_autoencoder = '/content/drive/MyDrive/ML final project/Trained_models/autoencoder_model_f32'\n",
        "\n",
        "# Load the models (note: no need for custom_objects here)\n",
        "encoder = tf.keras.models.load_model(filepath_encoder)\n",
        "decoder = tf.keras.models.load_model(filepath_decoder)\n",
        "autoencoder = tf.keras.models.load_model(filepath_autoencoder)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IDULJZAU0Upb",
        "outputId": "f7a78df2-fd76-424a-935b-1c254287fd59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Single Latent Vector:\n",
            "[  3.9139857  -78.696014    40.59769    100.93081      0.30083567\n",
            "  64.351006   -16.324448     7.0732713  -25.017939    18.731451\n",
            " -15.498808    73.33091    -37.94125     36.51313     33.477234\n",
            " -17.841972   -25.454254     1.874886    71.60545     14.626062\n",
            "  31.130869   -33.50864    -48.530712   -74.159355    15.654098\n",
            " 104.72894    -36.292656   -17.534327    72.05762     -0.9311351\n",
            "   7.9609513   52.66259   ]\n",
            "\n",
            "Vector shape: (32,)\n",
            "\n",
            "Latent vector at index 5:\n",
            "[ 63.761765  -90.588715   29.164812   34.263607    9.835992   71.956505\n",
            "  13.545499   64.47211   -33.322178   30.295397  -35.62386     8.381276\n",
            " -71.50911    64.85791     4.0841    -43.360874  -35.39441    66.37679\n",
            "  69.46605    44.815323   -2.2411065 -39.047676  -54.15908   -87.708084\n",
            "  26.135551   88.97029   -26.898077    9.175446   82.1819     -4.3425303\n",
            " -35.106884   49.084946 ]\n",
            "\n",
            "Corresponding image filename: C1_S1_I6.tiff\n",
            "Eye color label: brown\n"
          ]
        }
      ],
      "source": [
        "# Load the stored latent vectors\n",
        "latent_vectors = np.load('/content/drive/MyDrive/ML final project/Trained_models/latent_vectors_f32.npy')\n",
        "\n",
        "# View a single latent vector (e.g., the first one)\n",
        "single_vector = latent_vectors[0]  # Gets the first vector\n",
        "print(\"Single Latent Vector:\")\n",
        "print(single_vector)\n",
        "\n",
        "# If you want to see its shape\n",
        "print(\"\\nVector shape:\", single_vector.shape)\n",
        "\n",
        "# If you want to view a specific index\n",
        "index = 5  # Change this to view different vectors\n",
        "specific_vector = latent_vectors[index]\n",
        "print(f\"\\nLatent vector at index {index}:\")\n",
        "print(specific_vector)\n",
        "\n",
        "# Optional: If you want to see it alongside its corresponding image label\n",
        "# Load the vector mapping file if you saved it\n",
        "vector_mapping = pd.read_csv('/content/drive/MyDrive/ML final project/Trained_models/vector_mapping_f32.csv')\n",
        "print(f\"\\nCorresponding image filename: {vector_mapping['filename'][index]}\")\n",
        "print(f\"Eye color label: {vector_mapping['label'][index]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez4ab8WUn3CY"
      },
      "source": [
        "# TESTING\n",
        "function for preprocessing image1 and image2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "L3d195zEn7Uc",
        "outputId": "41da7fbf-61c7-4b62-ef2f-9e23496bfbb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessed image shape: (128, 128, 3)\n",
            "Value range: [0.024, 0.976]\n",
            "Preprocessed image shape: (128, 128, 3)\n",
            "Value range: [0.000, 0.984]\n",
            "Preprocessed image shape: (128, 128, 3)\n",
            "Value range: [0.016, 0.953]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def preprocess_image(image):\n",
        "    \"\"\"Preprocesses an image for the encoder using PIL for better quality.\"\"\"\n",
        "    # Check if image is None\n",
        "    if image is None:\n",
        "        raise ValueError(\"Input image is None\")\n",
        "\n",
        "    # Convert BGR to RGB (OpenCV loads in BGR)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Convert NumPy array to PIL Image\n",
        "    try:\n",
        "        image_pil = Image.fromarray(image)\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting to PIL Image: {e}\")\n",
        "        print(f\"Image shape: {image.shape}, dtype: {image.dtype}\")\n",
        "        raise\n",
        "\n",
        "    # Resize using PIL with high-quality interpolation\n",
        "    image_resized = image_pil.resize((IMG_SIZE, IMG_SIZE), Image.LANCZOS)\n",
        "\n",
        "    # Convert back to NumPy array and normalize\n",
        "    image_resized = np.array(image_resized)\n",
        "\n",
        "    # Ensure float32 type and proper normalization\n",
        "    image_normalized = image_resized.astype(np.float32) / 255.0\n",
        "\n",
        "    # Debug prints\n",
        "    print(f\"Preprocessed image shape: {image_normalized.shape}\")\n",
        "    print(f\"Value range: [{image_normalized.min():.3f}, {image_normalized.max():.3f}]\")\n",
        "\n",
        "    return image_normalized\n",
        "\n",
        "# When loading individual images:\n",
        "def load_and_preprocess_single_image(image_path):\n",
        "    \"\"\"Load and preprocess a single image.\"\"\"\n",
        "    # Read image\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise ValueError(f\"Could not load image from {image_path}\")\n",
        "\n",
        "    # Preprocess\n",
        "    preprocessed = preprocess_image(image)\n",
        "    return preprocessed\n",
        "\n",
        "image1_preprocessed = preprocess_image(image1)\n",
        "image2_preprocessed = preprocess_image(image2)\n",
        "image3_preprocessed = preprocess_image(image3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNwlthDZs1G9"
      },
      "source": [
        "Function for preprocessing the larger set of test data. (combine with above function?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Du-HRNyFRtm5"
      },
      "outputs": [],
      "source": [
        "def load_test_data():\n",
        "    \"\"\"Load and preprocess test dataset\"\"\"\n",
        "    # Read and sort the CSV file\n",
        "    labels_df = pd.read_csv(TEST_LABELS_CSV)\n",
        "    labels_df['C'] = labels_df['filename'].str.extract(r'C(\\d+)')[0].astype(int)\n",
        "    labels_df['S'] = labels_df['filename'].str.extract(r'S(\\d+)')[0].astype(int)\n",
        "    labels_df['I'] = labels_df['filename'].str.extract(r'I(\\d+)')[0].astype(int)\n",
        "    labels_df = labels_df.sort_values(['C', 'S', 'I'])\n",
        "\n",
        "    # Load and preprocess images\n",
        "    image_data = []\n",
        "    for filename in labels_df['filename']:\n",
        "        image_path = os.path.join(TEST_IMAGE_FOLDER, filename)\n",
        "        if os.path.exists(image_path):\n",
        "            img = Image.open(image_path)\n",
        "            img = img.resize((IMG_SIZE, IMG_SIZE), Image.LANCZOS)\n",
        "            img_array = np.array(img)\n",
        "            img_array = img_array.astype(\"float32\") / 255.0\n",
        "            image_data.append(img_array)\n",
        "\n",
        "    image_data = np.array(image_data)\n",
        "    return image_data, labels_df\n",
        "\n",
        "def generate_latent_vectors(encoder, image_data):\n",
        "    \"\"\"Generate latent vectors for the test dataset\"\"\"\n",
        "    return encoder.predict(image_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LKrZqVTxs01y"
      },
      "outputs": [],
      "source": [
        "def preprocess_images(image_path):\n",
        "    \"\"\"Preprocesses an image for the encoder using PIL for better quality.\"\"\"\n",
        "    input_shape = encoder.input_shape[1:]  # Get input shape from the encoder\n",
        "\n",
        "    # Open and resize image using PIL with high-quality interpolation\n",
        "    image_pil = Image.open(image_path)\n",
        "    image_resized = image_pil.resize((input_shape[1], input_shape[0]), Image.LANCZOS)\n",
        "\n",
        "    # Convert back to NumPy array and normalize\n",
        "    image_resized = np.array(image_resized)\n",
        "    image_normalized = image_resized / 255.0\n",
        "    image_preprocessed = image_normalized.astype(np.float32)\n",
        "    return image_preprocessed\n",
        "\n",
        "def generate_test_data(folder_path):\n",
        "    \"\"\"Generates image_data and latent_vectors from a folder of images.\"\"\"\n",
        "    image_data = []\n",
        "    latent_vectors = []\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.tiff'):  # Adjust extensions as needed\n",
        "            image_path = os.path.join(folder_path, filename)\n",
        "\n",
        "            # Load the image first using cv2\n",
        "            image = cv2.imread(image_path)\n",
        "            if image is None:\n",
        "                print(f\"Failed to load image: {image_path}\")\n",
        "                continue\n",
        "\n",
        "            # Convert BGR to RGB\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "            # Preprocess the image\n",
        "            preprocessed_image = preprocess_image(image)\n",
        "\n",
        "            # Add preprocessed image to image_data\n",
        "            image_data.append(preprocessed_image)\n",
        "\n",
        "            # Get latent vector using the encoder\n",
        "            latent_vector = encoder.predict(np.expand_dims(preprocessed_image, axis=0))\n",
        "            latent_vectors.append(latent_vector.flatten())  # Flatten to 1D\n",
        "\n",
        "    return np.array(image_data), np.array(latent_vectors)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTIPzVk0ysAe"
      },
      "source": [
        "# This is used to normalize the embeddings, which he;ps with the decoding process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zpAXmyyDVJdr"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Normalize the latent vector to range 0 to 1 for better interpretability\n",
        "def scale_latent_vector(test_latent_vectors):\n",
        "    min_val = np.min(test_latent_vectors)\n",
        "    max_val = np.max(test_latent_vectors)\n",
        "\n",
        "    # Normalize to [0, 1] range\n",
        "    scaled_latent_vector = (test_latent_vectors - min_val) / (max_val - min_val)\n",
        "    return scaled_latent_vector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNZggyQVzAiu"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7JNo3JIRxJIX"
      },
      "outputs": [],
      "source": [
        "# WORKING\n",
        "# USE THOS\n",
        "\n",
        "# Function to reconstruct images from latent vectors\n",
        "def reconstruct_images(latent_vectors, num_images=5):\n",
        "    reconstructed = decoder.predict(latent_vectors[:num_images])\n",
        "    reconstructed = np.clip(reconstructed, 0, 1)\n",
        "\n",
        "    # Display original vs reconstructed images\n",
        "    plt.figure(figsize=(15, 3))\n",
        "    for i in range(num_images):\n",
        "        # Original image\n",
        "        plt.subplot(2, num_images, i + 1)\n",
        "        plt.imshow(test_image_data[i])\n",
        "        plt.axis('off')\n",
        "        if i == 0:\n",
        "            plt.title('Original')\n",
        "\n",
        "        # Reconstructed image\n",
        "        plt.subplot(2, num_images, num_images + i + 1)\n",
        "        plt.imshow(reconstructed[i])\n",
        "        plt.axis('off')\n",
        "        if i == 0:\n",
        "            plt.title('Reconstructed')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Function to interpolate between two images\n",
        "def interpolate_images(decoder, latent_vector1, latent_vector2, num_steps=10):\n",
        "    \"\"\"Interpolate between two latent vectors and generate images\"\"\"\n",
        "    alphas = np.linspace(0, 1, num_steps)\n",
        "    interpolated_vectors = []\n",
        "\n",
        "    for alpha in alphas:\n",
        "        interpolated_vector = (1 - alpha) * latent_vector1 + alpha * latent_vector2\n",
        "        interpolated_vectors.append(interpolated_vector)\n",
        "\n",
        "    interpolated_vectors = np.array(interpolated_vectors)\n",
        "    interpolated_images = decoder.predict(interpolated_vectors)\n",
        "\n",
        "    return interpolated_images\n",
        "\n",
        "def display_interpolation(interpolated_images, color1=None, color2=None):\n",
        "    \"\"\"Display interpolation results\"\"\"\n",
        "    plt.figure(figsize=(15, 3))\n",
        "    for i in range(len(interpolated_images)):\n",
        "        plt.subplot(1, len(interpolated_images), i + 1)\n",
        "        plt.imshow(interpolated_images[i])\n",
        "        plt.axis('off')\n",
        "        if i == 0 and color1:\n",
        "            plt.title(f'{color1}', loc='left')\n",
        "        elif i == len(interpolated_images)-1 and color2:\n",
        "            plt.title(f'{color2}', loc='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_c2hZjcB6qNi"
      },
      "outputs": [],
      "source": [
        "#print(os.path.exists(filepath))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R4ralnOz5-y"
      },
      "source": [
        "Functions to interpolate the first image found for each class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CXXNqgMX6rFU"
      },
      "outputs": [],
      "source": [
        "# USE THOS\n",
        "\n",
        "# WORKING\n",
        "def get_color_examples(labels_df, latent_vectors, color_name, n=1):\n",
        "    \"\"\"Get latent vectors for eyes of a specific color\"\"\"\n",
        "    color_indices = labels_df[labels_df['label'] == color_name].index[:n]\n",
        "    return latent_vectors[color_indices]\n",
        "\n",
        "def interpolate_color_sequence(decoder, latent_vectors, labels_df, colors=['brown', 'blue', 'hazel', 'green', 'gray'], steps_between=10):\n",
        "    \"\"\"Create a sequence of interpolations between different eye colors\"\"\"\n",
        "    # Get one example for each color\n",
        "    color_vectors = [get_color_examples(labels_df, latent_vectors, color, 1)[0] for color in colors]\n",
        "\n",
        "    plt.figure(figsize=(20, 4 * (len(colors)-1)))\n",
        "    current_plot = 1\n",
        "\n",
        "    # Interpolate between consecutive colors\n",
        "    interpolated_images_all = []  # Store all interpolated images\n",
        "    true_labels_all = []         # Store corresponding labels\n",
        "\n",
        "    for i in range(len(colors)-1):\n",
        "        print(f\"Interpolating from {colors[i]} to {colors[i+1]}...\")\n",
        "\n",
        "        # Create interpolation\n",
        "        alphas = np.linspace(0, 1, steps_between)\n",
        "        interpolated_vectors = []\n",
        "\n",
        "        for alpha in alphas:\n",
        "            interpolated_vector = (1 - alpha) * color_vectors[i] + alpha * color_vectors[i+1]\n",
        "            interpolated_vectors.append(interpolated_vector)\n",
        "\n",
        "        interpolated_vectors = np.array(interpolated_vectors)\n",
        "        interpolated_images = decoder.predict(interpolated_vectors)\n",
        "        interpolated_images = np.clip(interpolated_images, 0, 1)\n",
        "\n",
        "        interpolated_images_all.extend(interpolated_images)  # Add to the list\n",
        "        true_labels_all.extend([colors[i]] * steps_between)  # Add corresponding labels\n",
        "\n",
        "        # Plot this interpolation sequence\n",
        "        plt.subplot(len(colors)-1, 1, current_plot)\n",
        "        for j in range(steps_between):\n",
        "            plt.subplot(len(colors)-1, steps_between, (current_plot-1)*steps_between + j + 1)\n",
        "            plt.imshow(interpolated_images[j])\n",
        "            plt.axis('off')\n",
        "            if j == 0:\n",
        "                plt.title(f'{colors[i]}', loc='left')\n",
        "            elif j == steps_between-1:\n",
        "                plt.title(f'{colors[i+1]}', loc='right')\n",
        "\n",
        "        current_plot += 1\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return np.array(interpolated_images_all), np.array(true_labels_all) # Return the images and labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFJ7lUnN2D8U"
      },
      "source": [
        "# Function to call decoder to reconstruct images from vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gSbz2-LrQmOX"
      },
      "outputs": [],
      "source": [
        "def reconstruct_images(latent_vectors, num_images=5):\n",
        "    reconstructed = decoder.predict(latent_vectors[:num_images])\n",
        "    reconstructed = np.clip(reconstructed, 0, 1)\n",
        "\n",
        "    plt.figure(figsize=(15, 4))\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(2, num_images, i + 1)\n",
        "        plt.imshow(test_image_data[i])\n",
        "        plt.axis('off')\n",
        "        if i == 0:\n",
        "            plt.title('Original')\n",
        "\n",
        "        plt.subplot(2, num_images, num_images + i + 1)\n",
        "        plt.imshow(reconstructed[i])\n",
        "        plt.axis('off')\n",
        "        if i == 0:\n",
        "            plt.title('Reconstructed')\n",
        "        # Calculate Mean Squared Error (MSE)\n",
        "        mse = np.mean(np.square(test_image_data[:num_images] - reconstructed))\n",
        "        print(f\"Reconstruction MSE: {mse}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k00TLwe2JRW"
      },
      "source": [
        "# Function to interpolate two random images from different classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jrVcNHLNgqh4"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "# USE THOS\n",
        "\n",
        "def randomer_interpolater(decoder, latent_vectors, labels_df, colors=['brown', 'blue', 'hazel', 'green', 'gray'], steps_between=10):\n",
        "    \"\"\"Create random interpolations between different eye colors\"\"\"\n",
        "    color_vectors = []\n",
        "    selected_indices = []\n",
        "\n",
        "    for color in colors:\n",
        "        color_indices = labels_df[labels_df['label'] == color].index\n",
        "        if len(color_indices) > 0:\n",
        "            random_index = random.choice(color_indices)\n",
        "            color_vectors.append(latent_vectors[random_index])\n",
        "            selected_indices.append(random_index)\n",
        "        else:\n",
        "            print(f\"Warning: No images found for color {color}\")\n",
        "            return\n",
        "\n",
        "    plt.figure(figsize=(20, 5 * (len(colors)-1)))\n",
        "    current_plot = 1\n",
        "\n",
        "    for i in range(len(colors)-1):\n",
        "        print(f\"Interpolating from {colors[i]} to {colors[i+1]}...\")\n",
        "        interpolated_images = interpolate_images(\n",
        "            decoder,\n",
        "            color_vectors[i],\n",
        "            color_vectors[i+1],\n",
        "            steps_between\n",
        "        )\n",
        "\n",
        "        # Display original images and interpolation\n",
        "        for j in range(steps_between):\n",
        "            plt.subplot(len(colors)-1, steps_between, (current_plot-1)*steps_between + j + 1)\n",
        "            plt.imshow(interpolated_images[j])\n",
        "            plt.axis('off')\n",
        "            if j == 0:\n",
        "                plt.title(f'{colors[i]}', loc='left')\n",
        "            elif j == steps_between-1:\n",
        "                plt.title(f'{colors[i+1]}', loc='right')\n",
        "\n",
        "        current_plot += 1\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N1TkVPf2-ko"
      },
      "source": [
        "# Function to calculate how accurate the interpolation is.\n",
        "This is a good idea to show evaluation metrics, but it is not currently working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pniWiZ7RO6cu"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def calculate_reconstruction_metrics(original_images, reconstructed_images):\n",
        "    \"\"\"Calculate various reconstruction error metrics\"\"\"\n",
        "    # Mean Squared Error (MSE)\n",
        "    mse = mean_squared_error(original_images.reshape(len(original_images), -1),\n",
        "                           reconstructed_images.reshape(len(reconstructed_images), -1))\n",
        "\n",
        "    # Root Mean Squared Error (RMSE)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    # Mean Absolute Error (MAE)\n",
        "    mae = np.mean(np.abs(original_images - reconstructed_images))\n",
        "\n",
        "    # Peak Signal-to-Noise Ratio (PSNR)\n",
        "    max_pixel = 1.0\n",
        "    psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
        "\n",
        "    return {\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'PSNR': psnr\n",
        "    }\n",
        "\n",
        "def plot_tsne_visualization(latent_vectors, labels_df, perplexity=30):\n",
        "    \"\"\"Create t-SNE visualization of the latent space\"\"\"\n",
        "    # Perform t-SNE dimensionality reduction\n",
        "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
        "    latent_2d = tsne.fit_transform(latent_vectors)\n",
        "\n",
        "    # Create DataFrame for plotting\n",
        "    df = pd.DataFrame({\n",
        "        'x': latent_2d[:, 0],\n",
        "        'y': latent_2d[:, 1],\n",
        "        'color': labels_df['label']\n",
        "    })\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.scatterplot(data=df, x='x', y='y', hue='color', alpha=0.6)\n",
        "    plt.title('t-SNE Visualization of Latent Space')\n",
        "    plt.xlabel('t-SNE Component 1')\n",
        "    plt.ylabel('t-SNE Component 2')\n",
        "    plt.legend(title='Eye Color')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def calculate_interpolation_accuracy(interpolated_images, true_labels, color_mapping):\n",
        "    predicted_labels = []\n",
        "    for image in interpolated_images:\n",
        "        dominant_color = get_dominant_iris_color(image)\n",
        "        if dominant_color is not None:\n",
        "            min_dist = float('inf')\n",
        "            predicted_color = None\n",
        "\n",
        "            for color_name, color_value in color_mapping.items():\n",
        "                color1_rgb = sRGBColor(*dominant_color / 255.0)\n",
        "                color2_rgb = sRGBColor(*np.array(color_value) / 255.0)\n",
        "                color1_lab = convert_color(color1_rgb, LabColor)\n",
        "                color2_lab = convert_color(color2_rgb, LabColor)\n",
        "\n",
        "                delta_e = delta_e_cie2000(color1_lab, color2_lab)\n",
        "\n",
        "                if delta_e < min_dist:\n",
        "                    min_dist = delta_e\n",
        "                    predicted_color = color_name\n",
        "\n",
        "            predicted_labels.append(predicted_color)\n",
        "        else:\n",
        "            predicted_labels.append(None)\n",
        "\n",
        "    # Filter out None values (cases where iris was not detected)\n",
        "    valid_indices = [i for i, label in enumerate(predicted_labels) if label is not None]\n",
        "    predicted_labels = np.array(predicted_labels)[valid_indices]\n",
        "    true_labels = true_labels[valid_indices]\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = np.mean(predicted_labels == true_labels)\n",
        "    print(f\"Interpolation Accuracy: {accuracy}\")\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "import umap\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_multiple_visualizations(latent_vectors, labels_df):\n",
        "    # Set up the figure\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(20, 20))\n",
        "\n",
        "    # 1. UMAP\n",
        "    reducer = umap.UMAP(random_state=42)\n",
        "    umap_embedded = reducer.fit_transform(latent_vectors)\n",
        "\n",
        "    sns.scatterplot(\n",
        "        x=umap_embedded[:, 0],\n",
        "        y=umap_embedded[:, 1],\n",
        "        hue=labels_df['label'],\n",
        "        alpha=0.6,\n",
        "        ax=axes[0,0]\n",
        "    )\n",
        "    axes[0,0].set_title('UMAP Visualization')\n",
        "\n",
        "    # 2. PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_result = pca.fit_transform(latent_vectors)\n",
        "\n",
        "    sns.scatterplot(\n",
        "        x=pca_result[:, 0],\n",
        "        y=pca_result[:, 1],\n",
        "        hue=labels_df['label'],\n",
        "        alpha=0.6,\n",
        "        ax=axes[0,1]\n",
        "    )\n",
        "    axes[0,1].set_title(f'PCA Visualization\\nExplained variance: {pca.explained_variance_ratio_.sum():.2%}')\n",
        "\n",
        "    # 3. Distribution of latent dimensions\n",
        "    # Take first 10 dimensions for visualization\n",
        "\n",
        "    latent_df = pd.DataFrame(latent_vectors[:, :10],\n",
        "                           columns=[f'Dim_{i+1}' for i in range(10)])\n",
        "\n",
        "    sns.boxplot(data=latent_df, ax=axes[1,0])\n",
        "    axes[1,0].set_title('Distribution of 8 Latent Dimensions')\n",
        "    axes[1,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # 4. Correlation matrix of first 10 latent dimensions\n",
        "    corr_matrix = latent_df.iloc[:, :10].corr()\n",
        "    sns.heatmap(corr_matrix, ax=axes[1,1], cmap='coolwarm', center=0)\n",
        "    axes[1,1].set_title('Correlation Matrix of Latent Dimensions')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Additional: Class distribution\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.countplot(data=labels_df, x='label')\n",
        "    plt.title('Distribution of Eye Colors in Dataset')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5CpjTdl3HqT"
      },
      "source": [
        "# Function to plot 20 random images\n",
        "This function needs some tweaking, but it is great for visualizing the dataset, and ensuring that labels are attached to the appropriate image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tlFf8Sra4qVT"
      },
      "outputs": [],
      "source": [
        "# TESTING\n",
        "def plot_random_samples(test_data, idx_to_color):\n",
        "    \"\"\"Plot 20 random samples from the test data in a 4x5 grid\"\"\"\n",
        "    # Select 20 random indices\n",
        "    indices = np.random.choice(len(test_data), 20, replace=False)\n",
        "\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    for i, idx in enumerate(indices):\n",
        "        # Get image and label\n",
        "        image, label = test_data[idx]\n",
        "\n",
        "        # Denormalize the image\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n",
        "        std = torch.tensor([0.229, 0.224, 0.225]).reshape(3, 1, 1)\n",
        "        img_denorm = image * std + mean\n",
        "\n",
        "        # Plot\n",
        "        plt.subplot(4, 5, i + 1)\n",
        "        plt.imshow(img_denorm.permute(1, 2, 0).clamp(0, 1))\n",
        "        plt.title(idx_to_color[label.item()], pad=5)\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlG3tNpi3ScY"
      },
      "source": [
        "# Main function\n",
        "\n",
        "explaination:\n",
        "- loads the label csv file. If you use other dataset, change this\n",
        "- color mapping: This is used for any future plotting, so that points can be colored according to their color class.\n",
        "- reconstruct images: displays the first five images, then reconstructs them via the decoder.\n",
        "- interpolating: there are two function calls, and one is commented out. If you want to interpolate from the same images each time you run this main, uncomment the first function call, and comment out the randomer_interpolater function. If you want to see random image interpolation, do the opposite.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx_oR_uZRjxp"
      },
      "outputs": [],
      "source": [
        "    print(\"Loading test data...\")\n",
        "    test_image_data, test_labels_df = load_test_data()\n",
        "    print(f\"Loaded {len(test_image_data)} test images\")\n",
        "    # Generate latent vectors for test data\n",
        "    print(\"Generating latent vectors...\")\n",
        "    test_latent_vectors = generate_latent_vectors(encoder, test_image_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jKIQhzZyGec"
      },
      "outputs": [],
      "source": [
        "labels_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_vdzFVjyeWg"
      },
      "outputs": [],
      "source": [
        "# prompt: Using dataframe labels_df: print the images of the eyes        1238, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383,\n",
        "\n",
        "# Convert the string of image indices to a list of integers.\n",
        "image_indices = [1238, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383]\n",
        "\n",
        "# Assuming the filenames are in sequential order in the dataframe.\n",
        "# Extract the filenames for the specified image indices from the dataframe.\n",
        "\n",
        "if not all(0 <= i < len(labels_df) for i in image_indices):\n",
        "    raise IndexError(\"Image indices are out of the valid range.\")\n",
        "\n",
        "# Extract filenames.\n",
        "filenames_to_print = labels_df.iloc[image_indices]['filename'].tolist()\n",
        "\n",
        "# Now you can use filenames_to_print to access and print the images.\n",
        "# Note: This is a placeholder; you'll need to provide the code to load and display the images.\n",
        "for filename in filenames_to_print:\n",
        "    print(f\"Displaying image: {filename}\")\n",
        "    # Add your image loading and display code here.  For example:\n",
        "    # from PIL import Image\n",
        "    image = Image.open(f\"/content/drive/MyDrive/ML final project/datasets/EYE_IMAGES_FULL/EYE_IMAGES_FULL/{filename}\")\n",
        "    image.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bK3eCXj0QsRo"
      },
      "outputs": [],
      "source": [
        "# USE THOS\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the labels\n",
        "    labels_df = pd.read_csv('/content/drive/MyDrive/ML final project/datasets/test_labels.csv')\n",
        "\n",
        "    color_mapping = {\n",
        "        'brown': [165, 42, 42],  # Example RGB for brown\n",
        "        'blue': [0, 0, 255],    # Example RGB for blue\n",
        "        'hazel': [184, 134, 11],  # Example RGB for hazel\n",
        "        'green': [0, 128, 0]   # Example RGB for green\n",
        "    }\n",
        "\n",
        "    print(\"Reconstructing sample images...\")\n",
        "    reconstruct_images(test_latent_vectors)\n",
        "\n",
        "    print(\"\\nInterpolating between different eye colors...\")\n",
        "    interpolate_color_sequence(decoder, latent_vectors, labels_df)\n",
        "    randomer_interpolater(decoder, test_latent_vectors, test_labels_df)\n",
        "\n",
        "    interpolated_images, true_labels = interpolate_color_sequence(decoder, latent_vectors, labels_df)\n",
        "    #interpolation_accuracy = calculate_interpolation_accuracy(interpolated_images, true_labels, color_mapping)\n",
        "\n",
        "    #print(\"\\nFinding best representative images for each color class...\")\n",
        "   # find_best_representative_images(image_data, labels_df)\n",
        "\n",
        "    #print(\"\\nExperimenting with hue...\")\n",
        "    #interpolate_and_apply_color_from_classes(find_best_representative_images(image_data, labels_df), image_data, labels_df)\n",
        "\n",
        "    # Calculate reconstruction metrics\n",
        "    print(\"\\nCalculating reconstruction metrics...\")\n",
        "    reconstructed_images = decoder.predict(test_latent_vectors)\n",
        "    metrics = calculate_reconstruction_metrics(test_image_data, reconstructed_images)\n",
        "\n",
        "    print(\"\\nReconstruction Metrics:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    # Visualize latent space using t-SNE\n",
        "    print(\"\\nGenerating t-SNE visualization...\")\n",
        "    plot_tsne_visualization(test_latent_vectors, test_labels_df)\n",
        "\n",
        "    # Add to your main section:\n",
        "    print(\"\\nGenerating additional visualizations...\")\n",
        "    plot_multiple_visualizations(test_latent_vectors, test_labels_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN3fFIr-4Pwc"
      },
      "source": [
        "# Result of interpolate_color_sequence(decoder, latent_vectors, labels_df)\n",
        "# NOTE: These results were also using the filtered iris dataset. If you want to reproduce this, you must load the proper dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWponRnkpqxo"
      },
      "outputs": [],
      "source": [
        "def interpolate_two_images(latent_vector1, latent_vector2, num_steps=10):\n",
        "    alphas = np.linspace(0, 1, num_steps)\n",
        "    interpolated_vectors = []\n",
        "\n",
        "    for alpha in alphas:\n",
        "        interpolated_vector = (1 - alpha) * latent_vector1 + alpha * latent_vector2\n",
        "        interpolated_vectors.append(interpolated_vector)\n",
        "\n",
        "    # Reshape the interpolated_vectors to (num_steps, 64)\n",
        "    interpolated_vectors = np.array(interpolated_vectors).reshape(num_steps, latent_vector1.shape[-1]) # Reshape to match decoder input\n",
        "    interpolated_images = decoder.predict(interpolated_vectors)\n",
        "\n",
        "    # Display interpolation results\n",
        "    plt.figure(figsize=(15, 3))\n",
        "    for i in range(num_steps):\n",
        "        plt.subplot(1, num_steps, i + 1)\n",
        "        plt.imshow(interpolated_images[i])\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asbv1nMIri1n"
      },
      "source": [
        "# TESTING GROUP MEMBER EYES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-74IOK0oXmS"
      },
      "outputs": [],
      "source": [
        "latent_vector1 = encoder.predict(np.expand_dims(image1_preprocessed, axis=0))\n",
        "latent_vector2 = encoder.predict(np.expand_dims(image2_preprocessed, axis=0))\n",
        "latent_vector3 = encoder.predict(np.expand_dims(image3_preprocessed, axis=0))\n",
        "\n",
        "\n",
        "interpolate_two_images(latent_vector1, latent_vector2, num_steps=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWy4AXbylUP-"
      },
      "source": [
        "# GRADIO TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNdn1_z0kraZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def greet(name, intensity):\n",
        "    return \"Hello, \" + name + \"!\" * int(intensity)\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=greet,\n",
        "    inputs=[\"image\"],\n",
        "    outputs=[\"text\"],\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyUnCAKIlNIR"
      },
      "outputs": [],
      "source": [
        "def create_interpolation(img1, img2):\n",
        "    \"\"\"Handle the Gradio interface and create interpolation\"\"\"\n",
        "    try:\n",
        "        # Directly normalize the input images without color space conversion\n",
        "        img1_preprocessed = np.array(img1).astype(np.float32) / 255.0\n",
        "        img2_preprocessed = np.array(img2).astype(np.float32) / 255.0\n",
        "\n",
        "        # Resize if needed\n",
        "        if img1_preprocessed.shape[:2] != (IMG_SIZE, IMG_SIZE):\n",
        "            img1_preprocessed = cv2.resize(img1_preprocessed, (IMG_SIZE, IMG_SIZE))\n",
        "        if img2_preprocessed.shape[:2] != (IMG_SIZE, IMG_SIZE):\n",
        "            img2_preprocessed = cv2.resize(img2_preprocessed, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "        # Get latent vectors\n",
        "        latent1 = encoder.predict(np.expand_dims(img1_preprocessed, axis=0))\n",
        "        latent2 = encoder.predict(np.expand_dims(img2_preprocessed, axis=0))\n",
        "\n",
        "        # Create interpolation vectors\n",
        "        num_steps = 10\n",
        "        alphas = np.linspace(0, 1, num_steps)\n",
        "        interpolated_vectors = [(1 - alpha) * latent1[0] + alpha * latent2[0] for alpha in alphas]\n",
        "\n",
        "        # Generate images from interpolated vectors\n",
        "        interpolated_images = decoder.predict(np.array(interpolated_vectors))\n",
        "\n",
        "        # Post-process images\n",
        "        interpolated_images = np.clip(interpolated_images * 255, 0, 255).astype(np.uint8)\n",
        "\n",
        "        # Create GIF with forward and backward interpolation for smooth loop\n",
        "        frames = list(interpolated_images) + list(interpolated_images[::-1])\n",
        "\n",
        "        # Create GIF that loops 10 times\n",
        "        output_path = \"interpolation.gif\"\n",
        "        imageio.mimsave(output_path, frames, duration=200, loop=10)\n",
        "\n",
        "        return output_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        raise gr.Error(str(e))\n",
        "\n",
        "\n",
        "# Create two tabs: one for file upload and one for webcam\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Eye Color Interpolation\")\n",
        "    gr.Markdown(\"Upload two eye images or use webcam to capture images and see the interpolation between them.\")\n",
        "\n",
        "    with gr.Tab(\"Upload Images\"):\n",
        "        with gr.Row():\n",
        "            image1_upload = gr.Image(type=\"numpy\", label=\"First Image\")\n",
        "            image2_upload = gr.Image(type=\"numpy\", label=\"Second Image\")\n",
        "        upload_button = gr.Button(\"Create Interpolation\")\n",
        "        output_upload = gr.Image(type=\"filepath\", label=\"Interpolation Result\")\n",
        "        upload_button.click(\n",
        "            create_interpolation,\n",
        "            inputs=[image1_upload, image2_upload],\n",
        "            outputs=output_upload\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"Webcam\"):\n",
        "        with gr.Row():\n",
        "            image1_webcam = gr.Image(type=\"numpy\", label=\"First Image\", sources=\"webcam\")\n",
        "            image2_webcam = gr.Image(type=\"numpy\", label=\"Second Image\", sources=\"webcam\")\n",
        "        webcam_button = gr.Button(\"Create Interpolation\")\n",
        "        output_webcam = gr.Image(type=\"filepath\", label=\"Interpolation Result\")\n",
        "        webcam_button.click(\n",
        "            create_interpolation,\n",
        "            inputs=[image1_webcam, image2_webcam],\n",
        "            outputs=output_webcam\n",
        "        )\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}