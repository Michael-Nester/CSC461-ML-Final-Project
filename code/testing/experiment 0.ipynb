{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["[link text](https://www.kaggle.com/datasets/hazemfahmy/eye-gaze-detection/data)\n"],"metadata":{"id":"9cuCC5iqp_XE"}},{"cell_type":"code","source":["from google.colab import files\n","files.upload()  # This will prompt you to upload kaggle.json\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"BaiR5_rrsNQb","executionInfo":{"status":"ok","timestamp":1732999237332,"user_tz":300,"elapsed":7010,"user":{"displayName":"Esra Bequir","userId":"17393950324623421753"}},"outputId":"19f57eb6-6083-4f77-972c-ca4fc564fd15"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-7a4814a5-5052-4833-b5eb-1ebc938b5229\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-7a4814a5-5052-4833-b5eb-1ebc938b5229\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kaggle.json to kaggle (1).json\n"]},{"output_type":"execute_result","data":{"text/plain":["{'kaggle (1).json': b'{\"username\":\"esra25\",\"key\":\"a415ec02e809410c89f784e2660ac6f7\"}'}"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["# Import required libraries\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, Flatten, Reshape, Conv2D, Conv2DTranspose\n","from tensorflow.keras.models import Model\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","import os\n","import json\n","from kaggle.api.kaggle_api_extended import KaggleApi\n","\n","# Move the kaggle.json to the correct location\n","kaggle_json_path = '/mnt/data/kaggle.json'\n","os.environ['KAGGLE_CONFIG_DIR'] = '/mnt/data/'\n","\n","# Use the Kaggle API to authenticate\n","api = KaggleApi()\n","api.authenticate()\n","\n","# Example: List datasets available to you or get the dataset\n","# api.dataset_list()  # Uncomment to list available datasets\n"],"metadata":{"id":"e8ploBxM9kdp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Set up Kaggle API credentials\n","os.environ['KAGGLE_CONFIG_DIR'] = '/mnt/data/'\n","\n","# Initialize and authenticate the Kaggle API\n","api = KaggleApi()\n","api.authenticate()\n","\n","# Define the dataset name\n","dataset_name = 'hazemfahmy/eye-gaze-detection'\n","\n","# Download and unzip the dataset\n","api.dataset_download_files(dataset_name, path='/mnt/data/', unzip=True)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oQThuth69pPG","executionInfo":{"status":"ok","timestamp":1733002224435,"user_tz":300,"elapsed":226159,"user":{"displayName":"Esra Bequir","userId":"17393950324623421753"}},"outputId":"ae8cf56a-75a5-4ce9-a8a9-58fb7d661267"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset URL: https://www.kaggle.com/datasets/hazemfahmy/eye-gaze-detection\n"]}]},{"cell_type":"code","source":["# Path to the TrainingSet directory containing the eye images\n","dataset_directory = '/mnt/data/TrainingSet'  # Adjusted to the correct folder\n","\n","# Load and preprocess the data from the TrainingSet directory\n","eye_images = load_and_preprocess_data(dataset_directory, target_size=(128, 128))\n","\n","# Proceed with the rest of the steps (defining the autoencoder, training, etc.)\n","\n","\n"],"metadata":{"id":"bqx4DuuiAxnH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Step 1: Load and preprocess the eye images from the Kaggle dataset\n","def load_and_preprocess_data(directory, target_size=(128, 128)):\n","    images = []\n","    for filename in os.listdir(directory):\n","        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Assuming images are in JPG or PNG format\n","            img_path = os.path.join(directory, filename)\n","            img = load_img(img_path, target_size=target_size)  # Load image and resize\n","            img_array = img_to_array(img)  # Convert image to array\n","            img_array = img_array.astype(\"float32\") / 255.0  # Normalize the image to [0, 1]\n","            images.append(img_array)\n","\n","    return np.array(images)\n","\n","# Load eye images from the Kaggle dataset (change this path to your actual directory)\n","dataset_directory = '/mnt/data/eye_gaze_data/eye-gaze-detection/TrainingSet'  # Replace this with your actual dataset path\n","eye_images = load_and_preprocess_data(dataset_directory, target_size=(128, 128))\n","\n","# Step 2: Define the autoencoder architecture\n","input_shape = (128, 128, 3)  # Input image size for color images (RGB)\n","input_img = Input(shape=input_shape)\n","\n","# Encoder\n","x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n","x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n","x = Flatten()(x)\n","latent_vector = Dense(128, activation='relu')(x)  # Latent space with 128 units\n","\n","# Decoder\n","x = Dense(128 * 128 * 64, activation='relu')(latent_vector)\n","x = Reshape((128, 128, 64))(x)\n","x = Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n","x = Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)\n","output_img = Conv2DTranspose(3, (3, 3), activation='sigmoid', padding='same')(x)\n","\n","# Define the autoencoder model\n","autoencoder = Model(inputs=input_img, outputs=output_img)\n","\n","# Compile the model with Mean Squared Error loss\n","autoencoder.compile(optimizer='adam', loss='mse')\n","\n","# Display the model summary\n","autoencoder.summary()\n","\n","# Step 3: Train the autoencoder (use eye images for both input and output)\n","history = autoencoder.fit(\n","    eye_images, eye_images,  # Use the same images for both input and output (autoencoder)\n","    epochs=10,\n","    batch_size=32,\n","    shuffle=True\n",")\n","\n","# Step 4: Visualize the results\n","def display_reconstructed_images(model, test_images, num_images=10):\n","    decoded_imgs = model.predict(test_images[:num_images])\n","    plt.figure(figsize=(20, 4))\n","    for i in range(num_images):\n","        # Original images\n","        ax = plt.subplot(2, num_images, i + 1)\n","        plt.imshow(test_images[i])\n","        plt.axis(\"off\")\n","\n","        # Reconstructed images\n","        ax = plt.subplot(2, num_images, i + 1 + num_images)\n","        plt.imshow(decoded_imgs[i])\n","        plt.axis(\"off\")\n","\n","    plt.show()\n","\n","# Display a few original and reconstructed images\n","display_reconstructed_images(autoencoder, eye_images)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"id":"T8dgNt4L86pl","executionInfo":{"status":"error","timestamp":1733002790421,"user_tz":300,"elapsed":142,"user":{"displayName":"Esra Bequir","userId":"17393950324623421753"}},"outputId":"26680bcf-1449-41b7-df93-9deac2226a3d"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/mnt/data/eye_gaze_data/eye-gaze-detection/TrainingSet'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-67-cf1323b06ebe>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Load eye images from the Kaggle dataset (change this path to your actual directory)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mdataset_directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/mnt/data/eye_gaze_data/eye-gaze-detection/TrainingSet'\u001b[0m  \u001b[0;31m# Replace this with your actual dataset path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0meye_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Step 2: Define the autoencoder architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-67-cf1323b06ebe>\u001b[0m in \u001b[0;36mload_and_preprocess_data\u001b[0;34m(directory, target_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_and_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Assuming images are in JPG or PNG format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/eye_gaze_data/eye-gaze-detection/TrainingSet'"]}]},{"cell_type":"code","source":["# Step 1: Create an ImageDataGenerator instance and resize images\n","train_datagen = ImageDataGenerator(rescale=1./255)  # Rescale pixel values to [0, 1]\n","\n","# Ensure images are resized to (128, 128) when loaded\n","train_generator = train_datagen.flow_from_directory(\n","    'eye_gaze_data/TrainingSet/',  # Path to your data folder\n","    target_size=(128, 128),        # Resize all images to (128, 128)\n","    batch_size=32,                 # Batch size for training\n","    class_mode=None,               # No labels for autoencoder\n","    shuffle=True                   # Shuffle data if needed\n",")\n","\n","# Step 2: Check the data being returned from the generator before training\n","for data in train_generator:\n","    if data is None:\n","        print(\"Found None in generator!\")\n","        break  # Stop if we find None\n","    print(\"Full batch shape:\", data[0].shape)  # Full shape of the data batch\n","    print(\"First image batch shape:\", data[0][0].shape)  # Shape of the first image\n","    print(\"Data type:\", type(data))  # Ensure it's a valid array or tensor\n","    break  # Stop after inspecting the first batch\n","\n","# Step 3: Define the autoencoder architecture\n","input_shape = (128, 128, 3)  # Input image size\n","input_img = Input(shape=input_shape)\n","\n","# Encoder\n","x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n","x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n","x = Flatten()(x)\n","latent_vector = Dense(128, activation='relu')(x)\n","\n","# Decoder\n","x = Dense(128 * 128 * 64, activation='relu')(latent_vector)\n","x = Reshape((128, 128, 64))(x)\n","x = Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n","x = Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)\n","output_img = Conv2DTranspose(3, (3, 3), activation='sigmoid', padding='same')(x)\n","\n","# Define the autoencoder model\n","autoencoder = Model(inputs=input_img, outputs=output_img)\n","\n","# Compile the model with Mean Squared Error loss\n","autoencoder.compile(optimizer='adam', loss='mse')\n","\n","# Display the model summary\n","autoencoder.summary()\n","\n","# Step 4: Proceed with training only if the generator is valid\n","history = autoencoder.fit(\n","    train_generator,  # Directly pass the generator\n","    steps_per_epoch=len(train_generator),\n","    epochs=10  # Adjust based on your needs\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":923},"id":"QQxGqLrD7PzI","executionInfo":{"status":"error","timestamp":1733001466488,"user_tz":300,"elapsed":5924,"user":{"displayName":"Esra Bequir","userId":"17393950324623421753"}},"outputId":"144a40d3-54e1-432e-96e4-2beac50bafc5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 61073 images belonging to 1 classes.\n","Full batch shape: (128, 128, 3)\n","First image batch shape: (128, 3)\n","Data type: <class 'numpy.ndarray'>\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional_4\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m896\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m18,496\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1048576\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │     \u001b[38;5;34m134,217,856\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1048576\u001b[0m)             │     \u001b[38;5;34m135,266,304\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ reshape_4 (\u001b[38;5;33mReshape\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_transpose_12                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m36,928\u001b[0m │\n","│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)                    │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_transpose_13                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m18,464\u001b[0m │\n","│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)                    │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_transpose_14                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │             \u001b[38;5;34m867\u001b[0m │\n","│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)                    │                             │                 │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1048576</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │     <span style=\"color: #00af00; text-decoration-color: #00af00\">134,217,856</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1048576</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">135,266,304</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ reshape_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_transpose_12                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)                    │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_transpose_13                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,464</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)                    │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_transpose_14                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">867</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)                    │                             │                 │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m269,559,811\u001b[0m (1.00 GB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269,559,811</span> (1.00 GB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m269,559,811\u001b[0m (1.00 GB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269,559,811</span> (1.00 GB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n"]},{"output_type":"error","ename":"ValueError","evalue":"None values not supported.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-55-ce500eadd749>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Step 4: Proceed with training only if the generator is valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m history = autoencoder.fit(\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Directly pass the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optree/ops.py\u001b[0m in \u001b[0;36mtree_map\u001b[0;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreespec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnone_is_leaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0mflat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreespec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrests\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtreespec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mflat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: None values not supported."]}]},{"cell_type":"code","source":["# Ensure the model is compiled before training\n","autoencoder.compile(optimizer='adam', loss='mse')\n","\n","# Check the data being returned from the generator before training\n","for data in train_generator:\n","    if data is None:\n","        print(\"Found None in generator!\")\n","        break  # Stop if we find None\n","    print(\"Full batch shape:\", data[0].shape)  # Full shape of the data batch\n","    print(\"First image batch shape:\", data[0][0].shape)  # Shape of the first image\n","    print(\"Data type:\", type(data))  # Ensure it's a valid array or tensor\n","    break  # Stop after inspecting the first batch\n","\n","# Proceed with training only if the generator is valid\n","history = autoencoder.fit(\n","    train_generator,\n","    steps_per_epoch=len(train_generator),\n","    epochs=10  # Adjust based on your needs\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"gh1oBWtP6hy_","executionInfo":{"status":"error","timestamp":1733001104008,"user_tz":300,"elapsed":6631,"user":{"displayName":"Esra Bequir","userId":"17393950324623421753"}},"outputId":"d3658339-b227-449d-c16b-1cef42cfb6c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Full batch shape: (32, 128, 128, 3)\n","First image batch shape: (128, 128, 3)\n","Data type: <class 'tuple'>\n","Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"error","ename":"InvalidArgumentError","evalue":"Graph execution error:\n\nDetected at node compile_loss/mse/sub defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-51-613eca89df37>\", line 15, in <cell line: 15>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 54, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/trainer.py\", line 359, in _compute_loss\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/trainer.py\", line 327, in compute_loss\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/compile_utils.py\", line 611, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/compile_utils.py\", line 652, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses/loss.py\", line 60, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py\", line 27, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py\", line 1303, in mean_squared_error\n\nIncompatible shapes: [32] vs. [32,128,128,3]\n\t [[{{node compile_loss/mse/sub}}]] [Op:__inference_one_step_on_iterator_4973]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-51-613eca89df37>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Proceed with training only if the generator is valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m history = autoencoder.fit(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node compile_loss/mse/sub defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-51-613eca89df37>\", line 15, in <cell line: 15>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 54, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/trainer.py\", line 359, in _compute_loss\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/trainer.py\", line 327, in compute_loss\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/compile_utils.py\", line 611, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/compile_utils.py\", line 652, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses/loss.py\", line 60, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py\", line 27, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py\", line 1303, in mean_squared_error\n\nIncompatible shapes: [32] vs. [32,128,128,3]\n\t [[{{node compile_loss/mse/sub}}]] [Op:__inference_one_step_on_iterator_4973]"]}]},{"cell_type":"code","source":["# Step 1: Create an ImageDataGenerator instance and resize images\n","train_datagen = ImageDataGenerator(rescale=1./255)  # Rescale pixel values to [0, 1]\n","\n","# Ensure images are resized to (128, 128) when loaded\n","train_generator = train_datagen.flow_from_directory(\n","    'eye_gaze_data/TrainingSet/',  # Path to your data folder\n","    target_size=(128, 128),        # Resize all images to (128, 128)\n","    batch_size=32,                 # Adjust batch size for training\n","    class_mode='binary'            # Adjust this based on your label format (e.g., 'binary' or 'categorical')\n",")\n","\n","# Step 2: Check the data being returned from the generator before training (This is your existing check)\n","for data in train_generator:\n","    if data is None:\n","        print(\"Found None in generator!\")\n","        break  # Stop if we find None\n","    print(\"Full batch shape:\", data[0].shape)  # Check full shape of the data batch\n","    print(\"First image batch shape:\", data[0][0].shape)  # Check the shape of the first image\n","    print(\"Data type:\", type(data))  # Ensure it's a valid array or tensor\n","    break  # Stop after inspecting the first batch\n","\n","# Step 3: Ensure train_generator is valid before proceeding to training\n","if train_generator is None:\n","    print(\"Error: train_generator is None!\")\n","else:\n","    # Proceed with training only if the generator is valid\n","    history = autoencoder.fit(\n","        train_generator,\n","        steps_per_epoch=len(train_generator),\n","        epochs=10  # Adjust based on your needs\n","    )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"id":"8hzBPtllvjnj","executionInfo":{"status":"error","timestamp":1733001024610,"user_tz":300,"elapsed":2963,"user":{"displayName":"Esra Bequir","userId":"17393950324623421753"}},"outputId":"5a1b4bce-cb8b-438b-85bd-2ac10898458c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 61073 images belonging to 1 classes.\n","Full batch shape: (32, 128, 128, 3)\n","First image batch shape: (128, 128, 3)\n","Data type: <class 'tuple'>\n","Epoch 1/10\n"]},{"output_type":"error","ename":"ValueError","evalue":"Creating variables on a non-first call to a function decorated with tf.function.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-4af5f0e82120>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Proceed with training only if the generator is valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     history = autoencoder.fit(\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mone_step_on_iterator\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;34m\"\"\"Runs a single training step given a Dataset iterator.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             outputs = self.distribute_strategy.run(\n\u001b[0m\u001b[1;32m    122\u001b[0m                 \u001b[0mone_step_on_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             )\n","\u001b[0;31mValueError\u001b[0m: Creating variables on a non-first call to a function decorated with tf.function."]}]},{"cell_type":"code","source":[],"metadata":{"id":"QQIJbvDrwLp0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ensure train_generator is not None\n","for data in train_generator:\n","    if data is None:\n","        print(\"Error: Found None data in generator\")\n","        continue  # This will skip the current batch and move to the next one\n","    print(\"Batch shape:\", data[0].shape)  # Print the shape of the image batch\n","    break  # Stop after inspecting the first batch\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OV_5noBZvNz4","executionInfo":{"status":"ok","timestamp":1732999403810,"user_tz":300,"elapsed":144,"user":{"displayName":"Esra Bequir","userId":"17393950324623421753"}},"outputId":"35eaa08e-dcf9-497e-fb05-fbe38911af9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Batch shape: (128, 128, 3)\n"]}]},{"cell_type":"markdown","source":["Create and Train an Autoencoder\n","\n","---\n","\n"],"metadata":{"id":"wdkgmRkPqKCC"}},{"cell_type":"code","source":["\n","\n","# Define the input shape\n","input_shape = (128, 128, 3)  # Adjust based on your image size\n","input_img = Input(shape=input_shape)\n","\n","# Encoder\n","x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n","x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n","x = Flatten()(x)\n","latent_vector = Dense(128, activation='relu')(x)\n","\n","# Decoder\n","x = Dense(128 * 128 * 64, activation='relu')(latent_vector)\n","x = Reshape((128, 128, 64))(x)\n","x = Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n","x = Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)\n","output_img = Conv2DTranspose(3, (3, 3), activation='sigmoid', padding='same')(x)\n","\n","# Define the autoencoder model\n","autoencoder = Model(inputs=input_img, outputs=output_img)\n","autoencoder.compile(optimizer='adam', loss='mse')\n","\n","# Display the model summary\n","autoencoder.summary()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"id":"r7oouwRBp3D5","executionInfo":{"status":"ok","timestamp":1732999825175,"user_tz":300,"elapsed":3994,"user":{"displayName":"Esra Bequir","userId":"17393950324623421753"}},"outputId":"53af2023-25b6-4ece-e7e5-f452d6f634b1"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional_1\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m896\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m18,496\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1048576\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │     \u001b[38;5;34m134,217,856\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1048576\u001b[0m)             │     \u001b[38;5;34m135,266,304\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_transpose_3 (\u001b[38;5;33mConv2DTranspose\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m36,928\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_transpose_4 (\u001b[38;5;33mConv2DTranspose\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m18,464\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_transpose_5 (\u001b[38;5;33mConv2DTranspose\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │             \u001b[38;5;34m867\u001b[0m │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1048576</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │     <span style=\"color: #00af00; text-decoration-color: #00af00\">134,217,856</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1048576</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">135,266,304</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_transpose_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_transpose_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,464</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_transpose_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">867</span> │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m269,559,811\u001b[0m (1.00 GB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269,559,811</span> (1.00 GB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m269,559,811\u001b[0m (1.00 GB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269,559,811</span> (1.00 GB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["Train the Autoencoder\n"],"metadata":{"id":"yr-Fyb_YqVcU"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# Initialize ImageDataGenerator for preprocessing\n","datagen = ImageDataGenerator(rescale=1.0 / 255.0)  # Normalize pixel values to [0, 1]\n","\n","# Define the path to the TrainingSet directory\n","training_set_path = './eye_gaze_data/TrainingSet/TrainingSet'\n","\n","# Create the generator\n","train_generator = datagen.flow_from_directory(\n","    training_set_path,\n","    target_size=(128, 128),  # Resize images to 128x128\n","    batch_size=32,          # Number of images per batch\n","    class_mode=None         # Autoencoder doesn't need class labels\n",")\n","\n","print(f\"Train generator created. Total samples: {train_generator.n}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPWmRp55uFJt","executionInfo":{"status":"ok","timestamp":1732999831373,"user_tz":300,"elapsed":2342,"user":{"displayName":"Esra Bequir","userId":"17393950324623421753"}},"outputId":"3f822039-1bd1-4827-b973-10b74dfa823d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 61073 images belonging to 8 classes.\n","Train generator created. Total samples: 61073\n"]}]},{"cell_type":"code","source":["print(f\"Number of samples in generator: {train_generator.n}\")\n","print(f\"Batch size: {train_generator.batch_size}\")\n","print(f\"Steps per epoch: {len(train_generator)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_oUKo-SBtLl0","executionInfo":{"status":"ok","timestamp":1732999847385,"user_tz":300,"elapsed":122,"user":{"displayName":"Esra Bequir","userId":"17393950324623421753"}},"outputId":"a160e43d-8d7a-4819-be06-0e95c802a681"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of samples in generator: 61073\n","Batch size: 32\n","Steps per epoch: 1909\n"]}]},{"cell_type":"code","source":["# Step 4: Train the autoencoder\n","# Check the data being returned from the generator before training\n","for data in train_generator:\n","    if data is None:\n","        print(\"Found None in generator!\")\n","        break  # Stop if we find None\n","    print(\"Batch shape:\", data[0].shape)  # Check shape of data (images)\n","    print(\"First image batch:\", data[0][0])  # Inspect the first image in the batch\n","    break  # Inspect only the first batch\n","\n","history = autoencoder.fit(\n","    train_generator,\n","    steps_per_epoch=len(train_generator),\n","    epochs=10  # Adjust based on your needs\n",")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"yutSCmkSqU3l","executionInfo":{"status":"error","timestamp":1733000073446,"user_tz":300,"elapsed":386,"user":{"displayName":"Esra Bequir","userId":"17393950324623421753"}},"outputId":"6b8e273f-2409-4206-d407-104fc447caf2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Batch shape: (128, 128, 3)\n","First image batch: [[0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]\n"," [0.38431376 0.38431376 0.38431376]]\n","Epoch 1/10\n"]},{"output_type":"error","ename":"ValueError","evalue":"None values not supported.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-e6e5ba31c88a>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Inspect only the first batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m history = autoencoder.fit(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optree/ops.py\u001b[0m in \u001b[0;36mtree_map\u001b[0;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreespec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnone_is_leaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0mflat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreespec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrests\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtreespec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mflat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: None values not supported."]}]}]}