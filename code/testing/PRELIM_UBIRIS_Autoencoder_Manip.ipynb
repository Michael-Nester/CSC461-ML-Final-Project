{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_n7hFoYxG-aW",
        "outputId": "c483d5c1-5b0d-42df-ba03-cd04d4aa2d14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "En4j9I5l7znF",
        "outputId": "e643888a-0eb2-4405-d3ee-f9e2a4b22462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           filename  label\n",
            "0   C261_S1_I1.tiff  brown\n",
            "1  C261_S1_I10.tiff  brown\n",
            "2  C261_S1_I11.tiff  brown\n",
            "3  C261_S1_I12.tiff  brown\n",
            "4  C261_S1_I13.tiff  brown\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5d0e5b482e9b>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Load the image, resize to 28x28 (or your desired size), and convert to array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Resize to 28x28\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m  \u001b[0;31m# Normalize to [0, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/image_utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         raise TypeError(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Reshape, Conv2D, Conv2DTranspose\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Paths to your data\n",
        "image_folder = '/content/drive/MyDrive/ML final project/datasets/CLASSES_400_300_Part2/'\n",
        "csv_path = '/content/drive/MyDrive/ML final project/datasets/iris_labelsShort.csv'\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "labels_df = pd.read_csv(csv_path)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(labels_df.head())\n",
        "\n",
        "# Use the correct column names from your CSV file\n",
        "image_names = labels_df['filename']\n",
        "labels = labels_df['label']\n",
        "\n",
        "# Load and preprocess images\n",
        "image_data = []\n",
        "image_labels = []\n",
        "\n",
        "for i, image_name in enumerate(image_names):\n",
        "    image_path = os.path.join(image_folder, image_name)\n",
        "\n",
        "    # Check if the image exists\n",
        "    if os.path.exists(image_path):\n",
        "        # Load the image, resize to 28x28 (or your desired size), and convert to array\n",
        "        img = load_img(image_path, target_size=(28, 28))  # Resize to 28x28\n",
        "        img_array = img_to_array(img)  # Convert to array\n",
        "        img_array = img_array.astype(\"float32\") / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "        # Append to lists\n",
        "        image_data.append(img_array)\n",
        "        image_labels.append(labels[i])  # Append corresponding label\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "image_data = np.array(image_data)\n",
        "image_labels = np.array(image_labels)\n",
        "\n",
        "print(f\"Image data shape: {image_data.shape}\")\n",
        "print(f\"Labels shape: {image_labels.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbBbtik-9RH_"
      },
      "outputs": [],
      "source": [
        "# Define the dimensions of the latent space\n",
        "latent_dim = 64  # Size of the compressed representation\n",
        "\n",
        "# Input shape\n",
        "input_shape = (28, 28, 3)  # Adjust this to match the size of your eye images\n",
        "\n",
        "# Build the Encoder\n",
        "input_img = Input(shape=input_shape, name=\"input_image\")\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = Flatten()(x)\n",
        "latent_vector = Dense(latent_dim, activation='relu', name=\"latent_vector\")(x)\n",
        "\n",
        "# Build the Decoder\n",
        "x = Dense(28 * 28 * 64, activation='relu')(latent_vector)\n",
        "x = Reshape((28, 28, 64))(x)\n",
        "x = Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)\n",
        "output_img = Conv2DTranspose(3, (3, 3), activation='sigmoid', padding='same', name=\"output_image\")(x)\n",
        "\n",
        "# Define the Autoencoder Model\n",
        "autoencoder = Model(inputs=input_img, outputs=output_img, name=\"autoencoder\")\n",
        "\n",
        "# Compile the Model\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Display the Model Summary\n",
        "autoencoder.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ck9pBPjRjTlU"
      },
      "outputs": [],
      "source": [
        "# Define the Autoencoder Model\n",
        "autoencoder = Model(inputs=input_img, outputs=output_img, name=\"autoencoder\")\n",
        "\n",
        "# Compile the Model\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Display the Model Summary\n",
        "autoencoder.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUaZDDkz9gRe"
      },
      "outputs": [],
      "source": [
        "#take two images get the vectors of them and then average them and output them to see what you get out\n",
        "# Train the Autoencoder with a binary cross-entropy-like loss\n",
        "history = autoencoder.fit(\n",
        "    image_data,  # Input images\n",
        "    image_data,  # Target images (same as input for autoencoder)\n",
        "    epochs=20,   # Number of epochs\n",
        "    batch_size=32,  # Adjust based on your memory constraints\n",
        "    validation_split=0.2,  # Use 20% of the data for validation\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Plot Training and Validation Loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title(\"Autoencoder Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}